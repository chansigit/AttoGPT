{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15a4862e-7467-4520-b549-ef79c35cb3c1",
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-09-09T00:52:26.759901Z",
     "iopub.status.busy": "2025-09-09T00:52:26.759673Z",
     "iopub.status.idle": "2025-09-09T00:52:35.106018Z",
     "shell.execute_reply": "2025-09-09T00:52:35.105693Z",
     "shell.execute_reply.started": "2025-09-09T00:52:26.759889Z"
    },
    "frozen": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dc0d40-77a0-4293-b638-acc332b8e40c",
   "metadata": {},
   "source": [
    "# Tokenizer utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa21d90a-1f5f-4036-8b7b-176cb08bd59d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T00:53:46.983158Z",
     "iopub.status.busy": "2025-09-09T00:53:46.982943Z",
     "iopub.status.idle": "2025-09-09T00:53:46.989309Z",
     "shell.execute_reply": "2025-09-09T00:53:46.989010Z",
     "shell.execute_reply.started": "2025-09-09T00:53:46.983144Z"
    }
   },
   "outputs": [],
   "source": [
    "class CharTokenizer:\n",
    "    def __init__(self, chars, add_unk=True):\n",
    "        self.add_unk = add_unk\n",
    "        # 可选的 UNK：放在 vocab[0]\n",
    "        if add_unk:\n",
    "            chars = [\"<unk>\"] + [c for c in chars if c != \"<unk>\"]\n",
    "        self.chars = list(chars)\n",
    "        self.stoi  = {ch: i for i, ch in enumerate(self.chars)}\n",
    "        self.itos  = {i: ch for i, ch in enumerate(self.chars)}\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.eos_id = None  # 字符级一般不用 eos\n",
    "\n",
    "    @classmethod\n",
    "    def build_from_text(cls, train_text, *, cover_val=None, cover_test=None, add_unk=True):\n",
    "        # 只用 train 建表；若想保证不 OOV，可把 val/test 的新字符并入：\n",
    "        vocab = sorted(list(set(train_text)))\n",
    "        if cover_val is not None:\n",
    "            vocab = sorted(list(set(vocab).union(set(cover_val))))\n",
    "        if cover_test is not None:\n",
    "            vocab = sorted(list(set(vocab).union(set(cover_test))))\n",
    "        return cls(vocab, add_unk=add_unk)\n",
    "\n",
    "    def encode(self, s: str):\n",
    "        if self.add_unk:\n",
    "            unk = self.stoi[\"<unk>\"]\n",
    "            return [self.stoi.get(c, unk) for c in s]\n",
    "        else:\n",
    "            # 无 unk 模式：确保外部不含未知字符\n",
    "            return [self.stoi[c] for c in s]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return \"\".join(self.itos.get(int(i), \"\") for i in ids)\n",
    "\n",
    "    def save(self, path: str):\n",
    "        obj = {\"chars\": self.chars, \"add_unk\": self.add_unk}\n",
    "        json.dump(obj, open(path, \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: str):\n",
    "        obj = json.load(open(path, \"r\", encoding=\"utf-8\"))\n",
    "        return cls(obj[\"chars\"], add_unk=obj.get(\"add_unk\", True))\n",
    "\n",
    "\n",
    "# ===== 3) 编码为 LongTensor，并落盘为 .bin（uint32 memmap）=====\n",
    "def encode_to_memmap(s: str, spath: str, tokenizer: CharTokenizer):\n",
    "    ids = np.array(tokenizer.encode(s), dtype=np.uint32)\n",
    "    m = np.memmap(spath, dtype=np.uint32, mode=\"w+\", shape=(ids.size,))\n",
    "    m[:] = ids\n",
    "    del m\n",
    "    return ids.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2f3b0c5-3295-465e-a2f2-da03d37abd25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T00:54:09.995317Z",
     "iopub.status.busy": "2025-09-09T00:54:09.995095Z",
     "iopub.status.idle": "2025-09-09T00:54:12.908494Z",
     "shell.execute_reply": "2025-09-09T00:54:12.908179Z",
     "shell.execute_reply.started": "2025-09-09T00:54:09.995303Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 66\n",
      "corpus sizes: 1003854 55770 55770\n"
     ]
    }
   ],
   "source": [
    "# 1) read 3 pieces of text（each with 1 row）\n",
    "ds_name = \"karpathy/tiny_shakespeare\"\n",
    "ds = load_dataset(ds_name)\n",
    "\n",
    "train_text = ds[\"train\"][0][\"text\"]\n",
    "val_text   = ds[\"validation\"][0][\"text\"]\n",
    "test_text  = ds[\"test\"][0][\"text\"]\n",
    "\n",
    "\n",
    "train_bin = \"data/train.bin\"\n",
    "val_bin   = \"data/val.bin\"\n",
    "test_bin  = \"data/test.bin\"\n",
    "\n",
    "# 选择：仅用 train 建表 + <unk> 兜底（推荐）\n",
    "tokenizer = CharTokenizer.build_from_text(train_text, add_unk=True)\n",
    "# 若你更想“完全覆盖 val/test 的字符”，改成：\n",
    "# tokenizer = CharTokenizer.build_from_text(train_text, cover_val=val_text, cover_test=test_text, add_unk=False)\n",
    "tok_path = \"data/char_tokenizer.json\"\n",
    "tokenizer.save(tok_path)\n",
    "\n",
    "train_tokens = encode_to_memmap(train_text, train_bin, tokenizer)\n",
    "val_tokens   = encode_to_memmap(val_text,   val_bin,   tokenizer)\n",
    "test_tokens  = encode_to_memmap(test_text,  test_bin,  tokenizer)\n",
    "\n",
    "# 保存元信息\n",
    "meta = {\n",
    "    \"tokenizer_type\": \"char\",\n",
    "    \"tokenizer_path\": tok_path,\n",
    "    \"vocab_size\": tokenizer.vocab_size,\n",
    "    \"has_unk\": tokenizer.add_unk,\n",
    "    \"unk_id\": tokenizer.stoi.get(\"<unk>\") if tokenizer.add_unk else None,\n",
    "    \"train_tokens\": int(train_tokens),\n",
    "    \"val_tokens\": int(val_tokens),\n",
    "    \"test_tokens\": int(test_tokens),\n",
    "}\n",
    "json.dump(meta, open(\"data/meta.json\", \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"vocab_size:\", tokenizer.vocab_size)\n",
    "print(\"corpus sizes:\", train_tokens, val_tokens, test_tokens)\n",
    "\n",
    "# ===== 4) 直接读回来作为 LongTensor（若你仍想在内存里调试）=====\n",
    "train_data = torch.from_numpy(np.memmap(train_bin, dtype=np.uint32, mode=\"r\").copy()).long()\n",
    "val_data   = torch.from_numpy(np.memmap(val_bin,   dtype=np.uint32, mode=\"r\").copy()).long()\n",
    "test_data  = torch.from_numpy(np.memmap(test_bin,  dtype=np.uint32, mode=\"r\").copy()).long()\n",
    "\n",
    "# （可选）安全自检：val/test 是否存在 OOV（仅在 add_unk=False 时有意义）\n",
    "if not tokenizer.add_unk:\n",
    "    assert set(val_text).issubset(set(train_text)) and set(test_text).issubset(set(train_text)), \\\n",
    "        \"val/test 含有 train 未见字符；要么加 <unk>，要么改为并入 val/test 字符建表\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d888811a-7756-472d-b3fd-606af0c58ced",
   "metadata": {},
   "source": [
    "# Load and prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec3976ae-0940-42c0-b426-672e7d4b94cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T00:54:23.705325Z",
     "iopub.status.busy": "2025-09-09T00:54:23.705088Z",
     "iopub.status.idle": "2025-09-09T00:54:23.708688Z",
     "shell.execute_reply": "2025-09-09T00:54:23.708382Z",
     "shell.execute_reply.started": "2025-09-09T00:54:23.705311Z"
    }
   },
   "outputs": [],
   "source": [
    "class RandomContiguousWindows(Dataset):\n",
    "    \"\"\"\n",
    "    get random continuous windows from a single long sequence. \n",
    "\n",
    "    To reduce sample-wise processing costs, we perform one `unfold` in the __init__\n",
    "    \n",
    "    This class returns a view of a shape \"(N-T) × (T+1)\" (no copying), __getitem__ only index once\n",
    "    \"\"\"\n",
    "    def __init__(self, src_1d_long: torch.Tensor, block_size: int, epoch_samples: int):\n",
    "        assert src_1d_long.dtype == torch.long and src_1d_long.dim() == 1\n",
    "        self.src = src_1d_long\n",
    "        self.T = block_size\n",
    "        self.length = epoch_samples  # 每个 epoch 想暴露多少“随机窗口”\n",
    "        # 预构建所有起点的窗口视图（注意：这是 as_strided 视图，不是复制）\n",
    "        # shape: (N - T) × (T+1)\n",
    "        self.windows = self.src.unfold(dimension=0, size=self.T + 1, step=1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # idx 实际只是“取多少次”的计数器，这里我们每次随机挑一个起点\n",
    "        i = torch.randint(0, self.windows.size(0), (1,)).item()\n",
    "        w = self.windows[i]     # (T+1,)\n",
    "        x = w[:-1]              # (T,)\n",
    "        y = w[1:]               # (T,)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a2e16af-9b51-459c-8518-ac8c090bb93a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T00:54:24.230258Z",
     "iopub.status.busy": "2025-09-09T00:54:24.230029Z",
     "iopub.status.idle": "2025-09-09T00:54:24.233397Z",
     "shell.execute_reply": "2025-09-09T00:54:24.233126Z",
     "shell.execute_reply.started": "2025-09-09T00:54:24.230246Z"
    }
   },
   "outputs": [],
   "source": [
    "block_size = 128\n",
    "train_ds = RandomContiguousWindows(train_data, block_size, epoch_samples=100000)\n",
    "val_ds   = RandomContiguousWindows(val_data,   block_size, epoch_samples=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c66251a-ac3f-46b0-859c-b2b73e5610ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T00:54:25.086511Z",
     "iopub.status.busy": "2025-09-09T00:54:25.086327Z",
     "iopub.status.idle": "2025-09-09T00:54:25.088566Z",
     "shell.execute_reply": "2025-09-09T00:54:25.088308Z",
     "shell.execute_reply.started": "2025-09-09T00:54:25.086499Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=False,\n",
    "                          num_workers=1, pin_memory=True, drop_last=True,\n",
    "                          prefetch_factor=2, persistent_workers=True)\n",
    "\n",
    "val_loader   = DataLoader(val_ds, batch_size=64, shuffle=False,\n",
    "                          num_workers=1, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40a8ad7-457c-452d-8e0d-1afb2248f3db",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64efd68-d115-4315-9854-7f4365a477a7",
   "metadata": {},
   "source": [
    "## DropPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8fafcdd-caf8-4fee-8e66-6e9e3daaa21e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T00:54:26.829276Z",
     "iopub.status.busy": "2025-09-09T00:54:26.829006Z",
     "iopub.status.idle": "2025-09-09T00:54:26.831843Z",
     "shell.execute_reply": "2025-09-09T00:54:26.831557Z",
     "shell.execute_reply.started": "2025-09-09T00:54:26.829265Z"
    }
   },
   "outputs": [],
   "source": [
    "class DropPath(nn.Module):\n",
    "    \"\"\"\n",
    "    Stochastic depth (a.k.a. DropPath) for residual branches.\n",
    "    Applies only during training.\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.drop_prob = float(drop_prob)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.drop_prob == 0.0 or not self.training:\n",
    "            return x\n",
    "        keep = 1.0 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "        mask = keep + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        mask.floor_()\n",
    "        return x / keep * mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262e673e-ac95-4f53-953d-0149f6a207af",
   "metadata": {},
   "source": [
    "## FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6f5b2c3-2faf-441e-ad91-f09a72740bba",
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-09-09T00:54:27.088235Z",
     "iopub.status.busy": "2025-09-09T00:54:27.088113Z",
     "iopub.status.idle": "2025-09-09T00:54:27.097298Z",
     "shell.execute_reply": "2025-09-09T00:54:27.097007Z",
     "shell.execute_reply.started": "2025-09-09T00:54:27.088225Z"
    },
    "frozen": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import spectral_norm as _spectral_norm\n",
    "\n",
    "\n",
    "def _round_to_multiple(x: int, base: int, mode: str = \"nearest\") -> int:\n",
    "    \"\"\"\n",
    "    Round integer x to a multiple of `base`.\n",
    "\n",
    "    Args:\n",
    "        x: Value to round.\n",
    "        base: The multiple to round to (e.g., 128, 256). If <=0, returns x.\n",
    "        mode: One of {\"nearest\", \"up\", \"down\"}.\n",
    "\n",
    "    Returns:\n",
    "        int: Rounded value.\n",
    "    \"\"\"\n",
    "    if base <= 0:\n",
    "        return x\n",
    "    if mode == \"up\":\n",
    "        return int(math.ceil(x / base) * base)\n",
    "    if mode == \"down\":\n",
    "        return int(math.floor(x / base) * base)\n",
    "    # nearest\n",
    "    down = int(math.floor(x / base) * base)\n",
    "    up = int(math.ceil(x / base) * base)\n",
    "    return up if (x - down) > (up - x) else down\n",
    "\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    \"\"\"\n",
    "    Flexible feed-forward network for Transformer blocks (no norm/residual inside).\n",
    "\n",
    "    Design:\n",
    "        - GLU family (\"swiglu\"/\"geglu\"): one Linear produces 2 * hidden_dim, split into\n",
    "          value and gate, then element-wise multiply, followed by the output projection.\n",
    "        - Plain two-layer FFN (\"gelu\"/\"silu\"): Linear -> activation -> Linear.\n",
    "        - No normalization, residual, or DropPath in this module; those belong outside.\n",
    "\n",
    "    Args:\n",
    "        d_model (int):\n",
    "            Input (and default output) channel size, typically the Transformer model width.\n",
    "        hidden_dim (int | None):\n",
    "            Intermediate width. If None, it is inferred based on `kind` and then aligned:\n",
    "              * kind in {\"gelu\", \"silu\"}   -> target = 4 * d_model\n",
    "              * kind in {\"swiglu\", \"geglu\"} -> target = (8/3) * d_model (≈ 2.67×)\n",
    "            The target is rounded to the nearest multiple of `align_to`\n",
    "            (default: 256 if d_model >= 1024 else 128).\n",
    "        out_dim (int | None):\n",
    "            Output channel size. Defaults to `d_model` (convenient for residual add).\n",
    "        kind (str):\n",
    "            Activation/topology: one of {\"swiglu\", \"geglu\", \"gelu\", \"silu\"}.\n",
    "            - \"swiglu\": gate uses SiLU\n",
    "            - \"geglu\" : gate uses GELU\n",
    "            - \"gelu\"  : plain two-layer FFN with GELU\n",
    "            - \"silu\"  : plain two-layer FFN with SiLU\n",
    "        dropout (float):\n",
    "            Dropout probability applied once on the FFN output. Default 0.0.\n",
    "        bias (bool):\n",
    "            Whether Linear layers use bias. Defaults to False (common in modern LLMs).\n",
    "        spectral_norm (bool):\n",
    "            If True, wrap Linear layers with spectral normalization. Default False.\n",
    "            (Not typically needed; reserved for special stability constraints.)\n",
    "        align_to (int | None):\n",
    "            Multiple to align `hidden_dim` to when auto-inferred. If None, uses\n",
    "            256 when d_model >= 1024, else 128.\n",
    "\n",
    "    Attributes:\n",
    "        d_model (int): Saved model width.\n",
    "        hidden_dim (int): Final intermediate width after inference/alignment.\n",
    "        out_dim (int): Final output width.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        hidden_dim: int | None = None,\n",
    "        out_dim: int | None = None,\n",
    "        kind: str = \"swiglu\",\n",
    "        dropout: float = 0.0,\n",
    "        bias: bool = False,\n",
    "        spectral_norm: bool = False,\n",
    "        align_to: int | None = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert kind in {\"swiglu\", \"geglu\", \"gelu\", \"silu\"}\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.kind = kind\n",
    "        self.out_dim = out_dim if out_dim is not None else d_model\n",
    "\n",
    "        # --- Auto-infer hidden_dim when not provided ---\n",
    "        if hidden_dim is None:\n",
    "            base = align_to if align_to is not None else (256 if d_model >= 1024 else 128)\n",
    "            if kind in {\"swiglu\", \"geglu\"}:\n",
    "                target = int(round((8.0 / 3.0) * d_model))  # ≈2.67× d_model\n",
    "            else:  # 'gelu' or 'silu'\n",
    "                target = 4 * d_model\n",
    "            hidden_dim = _round_to_multiple(target, base, mode=\"nearest\")\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # --- Layers ---\n",
    "        self.dropout = nn.Dropout(dropout) if (dropout and dropout > 0.0) else nn.Identity()\n",
    "\n",
    "        if kind in {\"swiglu\", \"geglu\"}:\n",
    "            # Produce 2*hidden_dim in one matmul; then split into (value, gate).\n",
    "            lin_in = nn.Linear(d_model, 2 * hidden_dim, bias=bias)\n",
    "            lin_out = nn.Linear(hidden_dim, self.out_dim, bias=bias)\n",
    "        else:  # 'gelu' or 'silu'\n",
    "            lin_in = nn.Linear(d_model, hidden_dim, bias=bias)\n",
    "            lin_out = nn.Linear(hidden_dim, self.out_dim, bias=bias)\n",
    "\n",
    "        if spectral_norm:\n",
    "            lin_in = _spectral_norm(lin_in)\n",
    "            lin_out = _spectral_norm(lin_out)\n",
    "\n",
    "        self.proj_in = lin_in\n",
    "        self.proj_out = lin_out\n",
    "\n",
    "        # Activation: used as gate (GLU) or mid-activation (two-layer FFN).\n",
    "        self.act = {\n",
    "            \"swiglu\": F.silu,   # gate activation\n",
    "            \"geglu\":  F.gelu,   # gate activation\n",
    "            \"gelu\":   F.gelu,   # mid activation\n",
    "            \"silu\":   F.silu,   # mid activation\n",
    "        }[kind]\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Xavier-uniform for Linear weights; zeros for biases.\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (..., d_model).\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape (..., out_dim). If out_dim == d_model, it fits residual add.\n",
    "        \"\"\"\n",
    "        if self.kind in {\"swiglu\", \"geglu\"}:\n",
    "            value, gate = self.proj_in(x).chunk(2, dim=-1)\n",
    "            z = value * self.act(gate)      # GLU-style gating\n",
    "        else:\n",
    "            z = self.act(self.proj_in(x))   # plain two-layer FFN mid-activation\n",
    "        out = self.proj_out(z)\n",
    "        return self.dropout(out)\n",
    "\n",
    "\n",
    "class TransformerFFNBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    FFN sublayer with Pre-Norm, residual add, and optional DropPath.\n",
    "    This block intentionally does NOT include attention; it's just the MLP piece.\n",
    "\n",
    "    x -> x + DropPath( FFN( Norm(x) ) )\n",
    "\n",
    "    Args:\n",
    "        d_model (int): Model width.\n",
    "        hidden_dim (int | None): Passed to FFN (same auto-infer rule applies if None).\n",
    "        norm_type (str): \"rmsnorm\" | \"layernorm\".\n",
    "        drop_path (float): Stochastic depth probability on the residual branch.\n",
    "        ffn_kind (str): Passed to FFN (\"swiglu\" | \"geglu\" | \"gelu\" | \"silu\").\n",
    "        ffn_dropout (float): Output dropout inside FFN.\n",
    "        bias (bool): Linear bias in FFN.\n",
    "        spectral_norm (bool): Spectral norm on FFN linears.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        hidden_dim: int | None,\n",
    "        *,\n",
    "        norm_type: str = \"rmsnorm\",\n",
    "        drop_path: float = 0.0,\n",
    "        ffn_kind: str = \"swiglu\",\n",
    "        ffn_dropout: float = 0.0,\n",
    "        bias: bool = False,\n",
    "        spectral_norm: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if norm_type == \"rmsnorm\":\n",
    "            self.norm = nn.RMSNorm(d_model, eps=1e-6)\n",
    "        elif norm_type == \"layernorm\":\n",
    "            self.norm = nn.LayerNorm(d_model, eps=1e-5)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown norm_type: {norm_type}\")\n",
    "\n",
    "        self.ffn = FFN(\n",
    "            d_model=d_model,\n",
    "            hidden_dim=hidden_dim,   # may be None -> auto-infer\n",
    "            out_dim=d_model,\n",
    "            kind=ffn_kind,\n",
    "            dropout=ffn_dropout,\n",
    "            bias=bias,\n",
    "            spectral_norm=spectral_norm,\n",
    "        )\n",
    "        self.drop_path = DropPath(drop_path) if drop_path and drop_path > 0.0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + self.drop_path(self.ffn(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8892dd95-5aba-417c-80a8-9469e1544f44",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e68c3462-47b6-4500-ab52-124ca28b0b29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T00:54:27.596593Z",
     "iopub.status.busy": "2025-09-09T00:54:27.596410Z",
     "iopub.status.idle": "2025-09-09T00:54:27.602678Z",
     "shell.execute_reply": "2025-09-09T00:54:27.602394Z",
     "shell.execute_reply.started": "2025-09-09T00:54:27.596582Z"
    }
   },
   "outputs": [],
   "source": [
    "import math, torch, torch.nn as nn, torch.nn.functional as F\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, dropout=0.1, max_seq_len=2048):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0\n",
    "        self.n_head   = n_head\n",
    "        self.head_dim = n_embd // n_head\n",
    "\n",
    "        # 线性层：一次性产生 qkv，然后输出投影\n",
    "        self.qkv  = nn.Linear(n_embd, 3 * n_embd, bias=False)\n",
    "        self.proj = nn.Linear(n_embd, n_embd,     bias=False)\n",
    "\n",
    "        self.attn_drop  = nn.Dropout(dropout)\n",
    "        self.resid_drop = nn.Dropout(dropout)\n",
    "\n",
    "        # 预先构建“因果上三角 mask”，复用；persistent=False 不随 ckpt 存盘（可选）\n",
    "        causal = torch.triu(\n",
    "            torch.ones(max_seq_len, max_seq_len, dtype=torch.bool),\n",
    "            diagonal=1\n",
    "        )\n",
    "        self.register_buffer(\"causal_mask\", causal, persistent=False)\n",
    "\n",
    "    def forward(self, x):  # x: (B, T, C)\n",
    "        B, T, C = x.shape\n",
    "        # 拆出 q,k,v 并重排到 (B, h, T, d)\n",
    "        qkv = self.qkv(x)                      # (B,T,3C)\n",
    "        q, k, v = qkv.split(C, dim=2)         # 各 (B,T,C)\n",
    "        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # 注意力分数 + 因果 mask（只取前 T×T 的切片）\n",
    "        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)   # (B,h,T,T)\n",
    "        att = att.masked_fill(self.causal_mask[:T, :T], float('-inf'))\n",
    "\n",
    "        # 数值更稳：softmax 用 float32，再 cast 回原 dtype\n",
    "        att = F.softmax(att.float(), dim=-1).to(q.dtype)\n",
    "        att = self.attn_drop(att)\n",
    "\n",
    "        y = att @ v                                # (B,h,T,d)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_drop(self.proj(y))          # (B,T,C)\n",
    "        return y\n",
    "\n",
    "class TransformerAttnBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention sublayer with Pre-Norm, residual add, and optional DropPath:\n",
    "        x -> x + DropPath( Attn( Norm(x) ) )\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        n_head: int,\n",
    "        *,\n",
    "        norm_type: str = \"rmsnorm\",     # \"rmsnorm\" | \"layernorm\"\n",
    "        attn_dropout: float = 0.0,      # dropout on attn weights / proj\n",
    "        drop_path: float = 0.0,         # stochastic depth prob\n",
    "        max_seq_len: int = 2048,\n",
    "        bias: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if norm_type == \"rmsnorm\":\n",
    "            self.norm = nn.RMSNorm(d_model, eps=1e-6)\n",
    "        elif norm_type == \"layernorm\":\n",
    "            self.norm = nn.LayerNorm(d_model, eps=1e-5)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown norm_type: {norm_type}\")\n",
    "\n",
    "        # 复用你之前的注意力实现（带缓存因果 mask 更省分配）\n",
    "        self.attn = CausalSelfAttention(\n",
    "            n_embd=d_model,\n",
    "            n_head=n_head,\n",
    "            dropout=attn_dropout,\n",
    "            max_seq_len=max_seq_len,\n",
    "        )\n",
    "        self.drop_path = DropPath(drop_path) if drop_path and drop_path > 0.0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + self.drop_path(self.attn(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e90d0e-bb1d-4234-9def-606bd9cd5566",
   "metadata": {},
   "source": [
    "## Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f76c7ea-fc44-405d-ac01-50c4e8cd2684",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T00:54:28.158480Z",
     "iopub.status.busy": "2025-09-09T00:54:28.158244Z",
     "iopub.status.idle": "2025-09-09T00:54:28.161472Z",
     "shell.execute_reply": "2025-09-09T00:54:28.161197Z",
     "shell.execute_reply.started": "2025-09-09T00:54:28.158470Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Full decoder block:\n",
    "        x -> x + DropPath( Attn( Norm(x) ) )\n",
    "          -> x -> x + DropPath( FFN(  Norm(x) ) )\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        n_head: int,\n",
    "        *,\n",
    "        max_seq_len: int = 2048,\n",
    "        norm_type: str = \"rmsnorm\",\n",
    "        # DropPath：建议随层数线性增大（见下“经验值”）\n",
    "        drop_path: float = 0.0,\n",
    "        # 注意力与 FFN 的内部超参：\n",
    "        attn_dropout: float = 0.0,\n",
    "        ffn_kind: str = \"swiglu\",          # \"swiglu\" | \"geglu\" | \"gelu\" | \"silu\"\n",
    "        ffn_hidden: int | None = None,     # None → 自动推断并按 128/256 对齐\n",
    "        ffn_dropout: float = 0.0,\n",
    "        bias: bool = False,\n",
    "        spectral_norm: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.attn_blk = TransformerAttnBlock(\n",
    "            d_model, n_head,\n",
    "            norm_type=norm_type,\n",
    "            attn_dropout=attn_dropout,\n",
    "            drop_path=drop_path,\n",
    "            max_seq_len=max_seq_len,\n",
    "            bias=bias,\n",
    "        )\n",
    "        self.ffn_blk = TransformerFFNBlock(\n",
    "            d_model=d_model,\n",
    "            hidden_dim=ffn_hidden,\n",
    "            norm_type=norm_type,\n",
    "            drop_path=drop_path,\n",
    "            ffn_kind=ffn_kind,\n",
    "            ffn_dropout=ffn_dropout,\n",
    "            bias=bias,\n",
    "            spectral_norm=spectral_norm,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.attn_blk(x)\n",
    "        x = self.ffn_blk(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb27414f-864c-42f3-adba-6a246dda3eb6",
   "metadata": {},
   "source": [
    "## GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db5909f2-1660-425b-a86e-bb6e7fa97526",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T00:54:28.922463Z",
     "iopub.status.busy": "2025-09-09T00:54:28.922293Z",
     "iopub.status.idle": "2025-09-09T00:54:28.929651Z",
     "shell.execute_reply": "2025-09-09T00:54:28.929378Z",
     "shell.execute_reply.started": "2025-09-09T00:54:28.922451Z"
    }
   },
   "outputs": [],
   "source": [
    "class TinyGPT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        n_layer: int = 12,\n",
    "        n_head: int  = 12,\n",
    "        n_embd: int  = 768,\n",
    "        block_size: int = 1024,\n",
    "        *,\n",
    "        norm_type: str = \"rmsnorm\",\n",
    "        drop_path_max: float = 0.0,        # e.g. 0.05~0.1 for deeper nets\n",
    "        ffn_kind: str = \"swiglu\",\n",
    "        ffn_hidden: int | None = None,     # None -> auto per your FFN\n",
    "        ffn_dropout: float = 0.0,\n",
    "        attn_dropout: float = 0.0,\n",
    "        bias: bool = False,\n",
    "        spectral_norm: bool = False,\n",
    "        emb_dropout: float = 0.0           # keep small for pretrain\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "\n",
    "        self.tok_emb = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_emb = nn.Embedding(block_size, n_embd)   # 简单位置；需要 RoPE 时替换\n",
    "        self.drop    = nn.Dropout(emb_dropout)\n",
    "\n",
    "        # linearly increasing DropPath across depth\n",
    "        def get_dp(i):  # i in [0, n_layer-1]\n",
    "            return drop_path_max * (i / max(1, n_layer - 1))\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(\n",
    "                d_model=n_embd, n_head=n_head,\n",
    "                max_seq_len=block_size,\n",
    "                norm_type=norm_type,\n",
    "                drop_path=get_dp(i),\n",
    "                attn_dropout=attn_dropout,\n",
    "                ffn_kind=ffn_kind,\n",
    "                ffn_hidden=ffn_hidden,\n",
    "                ffn_dropout=ffn_dropout,\n",
    "                bias=bias,\n",
    "                spectral_norm=spectral_norm,\n",
    "            )\n",
    "            for i in range(n_layer)\n",
    "        ])\n",
    "\n",
    "        self.ln_f = nn.RMSNorm(n_embd, eps=1e-6) if norm_type == \"rmsnorm\" else nn.LayerNorm(n_embd, eps=1e-5)\n",
    "\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "        # weight tying\n",
    "        self.lm_head.weight = self.tok_emb.weight\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Embedding):\n",
    "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    @property\n",
    "    def num_params(self): \n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "    def forward(self, idx, targets=None):          # idx: (B,T)\n",
    "        B, T = idx.shape\n",
    "        assert T <= self.block_size\n",
    "        tok = self.tok_emb(idx)                    # (B,T,C)\n",
    "        pos = self.pos_emb(torch.arange(T, device=idx.device, dtype=torch.long))  # (T,C)\n",
    "        x   = self.drop(tok + pos[None, :, :])     # (B,T,C)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)                   # (B,T,V)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                targets.view(-1)\n",
    "            )\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        self.eval()\n",
    "        idx = idx.to(next(self.parameters()).device)\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / max(temperature, 1e-6)\n",
    "            if top_k is not None and top_k > 0:\n",
    "                k = min(top_k, logits.size(-1))\n",
    "                v = torch.topk(logits, k, dim=-1).values\n",
    "                logits = logits.masked_fill(logits < v[:, [-1]], float('-inf'))\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_tok = torch.multinomial(probs, num_samples=1)   # (B,1)\n",
    "            idx = torch.cat([idx, next_tok], dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32b3a96c-0d05-4919-adfa-4a64f3107f4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T00:56:48.627558Z",
     "iopub.status.busy": "2025-09-09T00:56:48.627367Z",
     "iopub.status.idle": "2025-09-09T00:56:48.698973Z",
     "shell.execute_reply": "2025-09-09T00:56:48.698641Z",
     "shell.execute_reply.started": "2025-09-09T00:56:48.627547Z"
    }
   },
   "outputs": [],
   "source": [
    "model = TinyGPT( tokenizer.vocab_size, n_layer=8, n_head=4, n_embd=256, block_size=block_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fdfc907-b892-483d-8756-9eedc0f28142",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T00:56:49.765420Z",
     "iopub.status.busy": "2025-09-09T00:56:49.765241Z",
     "iopub.status.idle": "2025-09-09T00:56:49.779009Z",
     "shell.execute_reply": "2025-09-09T00:56:49.778701Z",
     "shell.execute_reply.started": "2025-09-09T00:56:49.765408Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Param #: 6.0833 M  Loss at rand initialization: 4.1207\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "# 从 train_loader 里取一批\n",
    "xb, yb = next(iter(train_loader))     # xb, yb: (B, T)\n",
    "xb = xb.to(device, non_blocking=True)\n",
    "yb = yb.to(device, non_blocking=True)\n",
    "\n",
    "_, loss = model(xb, yb)\n",
    "\n",
    "print(f\"Model Param #: {model.num_params/1e6:.4f} M\" , \n",
    "      f\" Loss at rand initialization: {float(loss):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c678054-fa5c-49ad-bb64-e75ba13a3a55",
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-09-09T00:56:50.353379Z",
     "iopub.status.busy": "2025-09-09T00:56:50.353238Z",
     "iopub.status.idle": "2025-09-09T00:57:22.374326Z",
     "shell.execute_reply": "2025-09-09T00:57:22.373866Z",
     "shell.execute_reply.started": "2025-09-09T00:56:50.353369Z"
    },
    "frozen": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step   10 | train loss 3.561 | lr 6.66667e-05 | tokens 81,920\n",
      "step   20 | train loss 3.426 | lr 0.000133333 | tokens 163,840\n",
      "step   30 | train loss 3.294 | lr 0.0002 | tokens 245,760\n",
      "step   40 | train loss 2.928 | lr 0.000266667 | tokens 327,680\n",
      "step   50 | train loss 2.726 | lr 0.0003 | tokens 409,600\n",
      "step   60 | train loss 2.647 | lr 0.0003 | tokens 491,520\n",
      "step   70 | train loss 2.583 | lr 0.0003 | tokens 573,440\n",
      "step   80 | train loss 2.523 | lr 0.0003 | tokens 655,360\n",
      "step   90 | train loss 2.482 | lr 0.0003 | tokens 737,280\n",
      "step  100 | train loss 2.433 | lr 0.0003 | tokens 819,200\n",
      "==> eval loss 2.463\n",
      "step  110 | train loss 2.430 | lr 0.0003 | tokens 901,120\n",
      "step  120 | train loss 2.388 | lr 0.0003 | tokens 983,040\n",
      "step  130 | train loss 2.378 | lr 0.0003 | tokens 1,064,960\n",
      "step  140 | train loss 2.363 | lr 0.0003 | tokens 1,146,880\n",
      "step  150 | train loss 2.356 | lr 0.0003 | tokens 1,228,800\n",
      "step  160 | train loss 2.335 | lr 0.0003 | tokens 1,310,720\n",
      "step  170 | train loss 2.313 | lr 0.0003 | tokens 1,392,640\n",
      "step  180 | train loss 2.279 | lr 0.0003 | tokens 1,474,560\n",
      "step  190 | train loss 2.229 | lr 0.0003 | tokens 1,556,480\n",
      "step  200 | train loss 2.227 | lr 0.0003 | tokens 1,638,400\n",
      "==> eval loss 2.246\n",
      "step  210 | train loss 2.202 | lr 0.0003 | tokens 1,720,320\n",
      "step  220 | train loss 2.155 | lr 0.0003 | tokens 1,802,240\n",
      "step  230 | train loss 2.141 | lr 0.0003 | tokens 1,884,160\n",
      "step  240 | train loss 2.101 | lr 0.0003 | tokens 1,966,080\n",
      "step  250 | train loss 2.094 | lr 0.0003 | tokens 2,048,000\n",
      "step  260 | train loss 2.034 | lr 0.0003 | tokens 2,129,920\n",
      "step  270 | train loss 2.000 | lr 0.0003 | tokens 2,211,840\n",
      "step  280 | train loss 2.020 | lr 0.0003 | tokens 2,293,760\n",
      "step  290 | train loss 1.938 | lr 0.0003 | tokens 2,375,680\n",
      "step  300 | train loss 1.898 | lr 0.0003 | tokens 2,457,600\n",
      "==> eval loss 2.002\n",
      "step  310 | train loss 1.883 | lr 0.0003 | tokens 2,539,520\n",
      "step  320 | train loss 1.875 | lr 0.0003 | tokens 2,621,440\n",
      "step  330 | train loss 1.814 | lr 0.0003 | tokens 2,703,360\n",
      "step  340 | train loss 1.762 | lr 0.0003 | tokens 2,785,280\n",
      "step  350 | train loss 1.810 | lr 0.0003 | tokens 2,867,200\n",
      "step  360 | train loss 1.814 | lr 0.0003 | tokens 2,949,120\n",
      "step  370 | train loss 1.744 | lr 0.0003 | tokens 3,031,040\n",
      "step  380 | train loss 1.734 | lr 0.0003 | tokens 3,112,960\n",
      "step  390 | train loss 1.734 | lr 0.0003 | tokens 3,194,880\n",
      "step  400 | train loss 1.704 | lr 0.0003 | tokens 3,276,800\n",
      "==> eval loss 1.818\n",
      "step  410 | train loss 1.698 | lr 0.0003 | tokens 3,358,720\n",
      "step  420 | train loss 1.637 | lr 0.0003 | tokens 3,440,640\n",
      "step  430 | train loss 1.621 | lr 0.0003 | tokens 3,522,560\n",
      "step  440 | train loss 1.683 | lr 0.0003 | tokens 3,604,480\n",
      "step  450 | train loss 1.634 | lr 0.0003 | tokens 3,686,400\n",
      "step  460 | train loss 1.615 | lr 0.0003 | tokens 3,768,320\n",
      "step  470 | train loss 1.649 | lr 0.0003 | tokens 3,850,240\n",
      "step  480 | train loss 1.604 | lr 0.0003 | tokens 3,932,160\n",
      "step  490 | train loss 1.591 | lr 0.0003 | tokens 4,014,080\n",
      "step  500 | train loss 1.551 | lr 0.0003 | tokens 4,096,000\n",
      "==> eval loss 1.711\n",
      "step  510 | train loss 1.561 | lr 0.0003 | tokens 4,177,920\n",
      "step  520 | train loss 1.575 | lr 0.0003 | tokens 4,259,840\n",
      "step  530 | train loss 1.563 | lr 0.0003 | tokens 4,341,760\n",
      "step  540 | train loss 1.576 | lr 0.0003 | tokens 4,423,680\n",
      "step  550 | train loss 1.551 | lr 0.0003 | tokens 4,505,600\n",
      "step  560 | train loss 1.503 | lr 0.0003 | tokens 4,587,520\n",
      "step  570 | train loss 1.520 | lr 0.0003 | tokens 4,669,440\n",
      "step  580 | train loss 1.543 | lr 0.0003 | tokens 4,751,360\n",
      "step  590 | train loss 1.495 | lr 0.0003 | tokens 4,833,280\n",
      "step  600 | train loss 1.499 | lr 0.0003 | tokens 4,915,200\n",
      "==> eval loss 1.633\n",
      "step  610 | train loss 1.449 | lr 0.0003 | tokens 4,997,120\n",
      "step  620 | train loss 1.540 | lr 0.0003 | tokens 5,079,040\n",
      "step  630 | train loss 1.444 | lr 0.0003 | tokens 5,160,960\n",
      "step  640 | train loss 1.456 | lr 0.0003 | tokens 5,242,880\n",
      "step  650 | train loss 1.438 | lr 0.0003 | tokens 5,324,800\n",
      "step  660 | train loss 1.458 | lr 0.0003 | tokens 5,406,720\n",
      "step  670 | train loss 1.476 | lr 0.0003 | tokens 5,488,640\n",
      "step  680 | train loss 1.434 | lr 0.0003 | tokens 5,570,560\n",
      "step  690 | train loss 1.444 | lr 0.0003 | tokens 5,652,480\n",
      "step  700 | train loss 1.450 | lr 0.0003 | tokens 5,734,400\n",
      "==> eval loss 1.590\n",
      "step  710 | train loss 1.405 | lr 0.0003 | tokens 5,816,320\n",
      "step  720 | train loss 1.406 | lr 0.0003 | tokens 5,898,240\n",
      "step  730 | train loss 1.406 | lr 0.0003 | tokens 5,980,160\n",
      "step  740 | train loss 1.431 | lr 0.0003 | tokens 6,062,080\n",
      "step  750 | train loss 1.426 | lr 0.0003 | tokens 6,144,000\n",
      "step  760 | train loss 1.381 | lr 0.0003 | tokens 6,225,920\n",
      "step  770 | train loss 1.408 | lr 0.0003 | tokens 6,307,840\n",
      "step  780 | train loss 1.415 | lr 0.0003 | tokens 6,389,760\n",
      "step  790 | train loss 1.410 | lr 0.0003 | tokens 6,471,680\n",
      "step  800 | train loss 1.398 | lr 0.0003 | tokens 6,553,600\n",
      "==> eval loss 1.539\n",
      "step  810 | train loss 1.381 | lr 0.0003 | tokens 6,635,520\n",
      "step  820 | train loss 1.396 | lr 0.0003 | tokens 6,717,440\n",
      "step  830 | train loss 1.403 | lr 0.0003 | tokens 6,799,360\n",
      "step  840 | train loss 1.362 | lr 0.0003 | tokens 6,881,280\n",
      "step  850 | train loss 1.387 | lr 0.0003 | tokens 6,963,200\n",
      "step  860 | train loss 1.340 | lr 0.0003 | tokens 7,045,120\n",
      "step  870 | train loss 1.388 | lr 0.0003 | tokens 7,127,040\n",
      "step  880 | train loss 1.373 | lr 0.0003 | tokens 7,208,960\n",
      "step  890 | train loss 1.383 | lr 0.0003 | tokens 7,290,880\n",
      "step  900 | train loss 1.382 | lr 0.0003 | tokens 7,372,800\n",
      "==> eval loss 1.525\n",
      "step  910 | train loss 1.328 | lr 0.0003 | tokens 7,454,720\n",
      "step  920 | train loss 1.351 | lr 0.0003 | tokens 7,536,640\n",
      "step  930 | train loss 1.343 | lr 0.0003 | tokens 7,618,560\n",
      "step  940 | train loss 1.311 | lr 0.0003 | tokens 7,700,480\n",
      "step  950 | train loss 1.367 | lr 0.0003 | tokens 7,782,400\n",
      "step  960 | train loss 1.353 | lr 0.0003 | tokens 7,864,320\n",
      "step  970 | train loss 1.362 | lr 0.0003 | tokens 7,946,240\n",
      "step  980 | train loss 1.308 | lr 0.0003 | tokens 8,028,160\n",
      "step  990 | train loss 1.342 | lr 0.0003 | tokens 8,110,080\n",
      "step 1000 | train loss 1.366 | lr 0.0003 | tokens 8,192,000\n",
      "==> eval loss 1.493\n",
      "step 1010 | train loss 1.333 | lr 0.0003 | tokens 8,273,920\n",
      "step 1020 | train loss 1.310 | lr 0.0003 | tokens 8,355,840\n",
      "step 1030 | train loss 1.318 | lr 0.0003 | tokens 8,437,760\n",
      "step 1040 | train loss 1.283 | lr 0.0003 | tokens 8,519,680\n",
      "step 1050 | train loss 1.310 | lr 0.0003 | tokens 8,601,600\n",
      "step 1060 | train loss 1.335 | lr 0.0003 | tokens 8,683,520\n",
      "step 1070 | train loss 1.310 | lr 0.0003 | tokens 8,765,440\n",
      "step 1080 | train loss 1.274 | lr 0.0003 | tokens 8,847,360\n",
      "step 1090 | train loss 1.307 | lr 0.0003 | tokens 8,929,280\n",
      "step 1100 | train loss 1.336 | lr 0.0003 | tokens 9,011,200\n",
      "==> eval loss 1.494\n",
      "step 1110 | train loss 1.298 | lr 0.0003 | tokens 9,093,120\n",
      "step 1120 | train loss 1.299 | lr 0.0003 | tokens 9,175,040\n",
      "step 1130 | train loss 1.292 | lr 0.0003 | tokens 9,256,960\n",
      "step 1140 | train loss 1.290 | lr 0.0003 | tokens 9,338,880\n",
      "step 1150 | train loss 1.302 | lr 0.0003 | tokens 9,420,800\n",
      "step 1160 | train loss 1.240 | lr 0.0003 | tokens 9,502,720\n",
      "step 1170 | train loss 1.304 | lr 0.0003 | tokens 9,584,640\n",
      "step 1180 | train loss 1.288 | lr 0.0003 | tokens 9,666,560\n",
      "step 1190 | train loss 1.249 | lr 0.0003 | tokens 9,748,480\n",
      "step 1200 | train loss 1.289 | lr 0.0003 | tokens 9,830,400\n",
      "==> eval loss 1.475\n",
      "step 1210 | train loss 1.269 | lr 0.0003 | tokens 9,912,320\n",
      "step 1220 | train loss 1.304 | lr 0.0003 | tokens 9,994,240\n",
      "step 1230 | train loss 1.270 | lr 0.0003 | tokens 10,076,160\n",
      "step 1240 | train loss 1.221 | lr 0.0003 | tokens 10,158,080\n",
      "step 1250 | train loss 1.260 | lr 0.0003 | tokens 10,240,000\n",
      "step 1260 | train loss 1.263 | lr 0.0003 | tokens 10,321,920\n",
      "step 1270 | train loss 1.226 | lr 0.0003 | tokens 10,403,840\n",
      "step 1280 | train loss 1.235 | lr 0.0003 | tokens 10,485,760\n",
      "step 1290 | train loss 1.288 | lr 0.0003 | tokens 10,567,680\n",
      "step 1300 | train loss 1.233 | lr 0.0003 | tokens 10,649,600\n",
      "==> eval loss 1.471\n",
      "step 1310 | train loss 1.269 | lr 0.0003 | tokens 10,731,520\n",
      "step 1320 | train loss 1.230 | lr 0.0003 | tokens 10,813,440\n",
      "step 1330 | train loss 1.285 | lr 0.0003 | tokens 10,895,360\n",
      "step 1340 | train loss 1.254 | lr 0.0003 | tokens 10,977,280\n",
      "step 1350 | train loss 1.223 | lr 0.0003 | tokens 11,059,200\n",
      "step 1360 | train loss 1.267 | lr 0.0003 | tokens 11,141,120\n",
      "step 1370 | train loss 1.224 | lr 0.0003 | tokens 11,223,040\n",
      "step 1380 | train loss 1.244 | lr 0.0003 | tokens 11,304,960\n",
      "step 1390 | train loss 1.240 | lr 0.0003 | tokens 11,386,880\n",
      "step 1400 | train loss 1.223 | lr 0.0003 | tokens 11,468,800\n",
      "==> eval loss 1.467\n",
      "step 1410 | train loss 1.209 | lr 0.0003 | tokens 11,550,720\n",
      "step 1420 | train loss 1.237 | lr 0.0003 | tokens 11,632,640\n",
      "step 1430 | train loss 1.258 | lr 0.0003 | tokens 11,714,560\n",
      "step 1440 | train loss 1.202 | lr 0.0003 | tokens 11,796,480\n",
      "step 1450 | train loss 1.224 | lr 0.0003 | tokens 11,878,400\n",
      "step 1460 | train loss 1.224 | lr 0.0003 | tokens 11,960,320\n",
      "step 1470 | train loss 1.232 | lr 0.0003 | tokens 12,042,240\n",
      "step 1480 | train loss 1.229 | lr 0.0003 | tokens 12,124,160\n",
      "step 1490 | train loss 1.191 | lr 0.0003 | tokens 12,206,080\n",
      "step 1500 | train loss 1.175 | lr 0.0003 | tokens 12,288,000\n",
      "==> eval loss 1.474\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from itertools import cycle\n",
    "\n",
    "max_steps     = 1500\n",
    "eval_interval = 100\n",
    "init_lr = 3e-4\n",
    "warmup_steps = max(1, int(0.03 * max_steps))\n",
    "\n",
    "def grouped_params(model, wd=0.1):\n",
    "    decay, no_decay = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if not p.requires_grad: continue\n",
    "        (decay if p.ndim >= 2 and ('norm' not in n.lower()) else no_decay).append(p)\n",
    "    return [{\"params\": decay, \"weight_decay\": wd},\n",
    "            {\"params\": no_decay, \"weight_decay\": 0.0}]\n",
    "\n",
    "optimizer = torch.optim.AdamW(grouped_params(model, wd=0.1),\n",
    "                              lr=init_lr, betas=(0.9,0.95), eps=1e-8)\n",
    "\n",
    "model.train()\n",
    "train_iter = cycle(train_loader)\n",
    "tokens_seen = 0\n",
    "\n",
    "for step in range(1, max_steps + 1):\n",
    "    xb, yb = next(train_iter)\n",
    "    xb = xb.to(device, non_blocking=True); yb = yb.to(device, non_blocking=True)\n",
    "\n",
    "    # 线性 warmup（3% 步数）\n",
    "    if step <= warmup_steps:\n",
    "        scale = step / warmup_steps\n",
    "        for g in optimizer.param_groups: g[\"lr\"] = init_lr * scale\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16, enabled=(device==\"cuda\")):\n",
    "        _, loss = model(xb, yb)\n",
    "\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    tokens_seen += xb.numel()\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        lr_now = optimizer.param_groups[0][\"lr\"]\n",
    "        print(f\"step {step:4d} | train loss {loss.item():.3f} | lr {lr_now:.6g} | tokens {tokens_seen:,}\")\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            vloss_total, vcount = 0.0, 0\n",
    "            for vx, vy in val_loader:\n",
    "                vx = vx.to(device, non_blocking=True); vy = vy.to(device, non_blocking=True)\n",
    "                _, vloss = model(vx, vy)\n",
    "                vloss_total += vloss.item(); vcount += 1\n",
    "        print(f\"==> eval loss {vloss_total / max(1,vcount):.3f}\")\n",
    "        model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12d23a90-611c-49ac-80bc-d5c1cf5693f0",
   "metadata": {
    "deletable": true,
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-09-09T00:57:22.375317Z",
     "iopub.status.busy": "2025-09-09T00:57:22.375144Z",
     "iopub.status.idle": "2025-09-09T00:57:22.382214Z",
     "shell.execute_reply": "2025-09-09T00:57:22.381924Z",
     "shell.execute_reply.started": "2025-09-09T00:57:22.375302Z"
    },
    "frozen": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Optional, Callable, Union\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def stream_generate(\n",
    "    model,\n",
    "    tokenizer: Optional[object] = None,\n",
    "    prompt: Union[str, List[int]] = \"\",\n",
    "    *,\n",
    "    max_new_tokens: int = 128,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: Optional[int] = None,\n",
    "    top_p: Optional[float] = None,\n",
    "    repetition_penalty: float = 1.0,\n",
    "    eos_id: Optional[int] = None,\n",
    "    device: str = \"cuda\",\n",
    "    print_every: int = 1,\n",
    "):\n",
    "    \"\"\"\n",
    "    边生成边打印。返回完整字符串和 id。\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = device if torch.cuda.is_available() and device.startswith(\"cuda\") else \"cpu\"\n",
    "    model = model.to(device)\n",
    "\n",
    "    if isinstance(prompt, str):\n",
    "        if tokenizer is None:\n",
    "            raise ValueError(\"prompt 是字符串时需要提供 tokenizer（含 encode/decode）。\")\n",
    "        ids = tokenizer.encode(prompt)\n",
    "        print(prompt, end=\"\", flush=True)\n",
    "    else:\n",
    "        ids = list(prompt)\n",
    "\n",
    "    x = torch.tensor([ids], dtype=torch.long, device=device)\n",
    "    collected_new_ids = []\n",
    "\n",
    "    for t in range(max_new_tokens):\n",
    "        block_size = getattr(model, \"block_size\", x.size(1))\n",
    "        x_cond = x[:, -block_size:]\n",
    "\n",
    "        logits, _ = model(x_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        if temperature and temperature > 0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "        if repetition_penalty != 1.0 and x.numel() > 0:\n",
    "            unique_tokens = torch.unique(x)\n",
    "            logits[:, unique_tokens] /= repetition_penalty\n",
    "\n",
    "        if top_k is not None and top_k > 0:\n",
    "            k = min(top_k, logits.size(-1))\n",
    "            v = torch.topk(logits, k, dim=-1).values[:, [-1]]\n",
    "            logits = logits.masked_fill(logits < v, float(\"-inf\"))\n",
    "\n",
    "        if top_p is not None and 0.0 < top_p < 1.0:\n",
    "            sorted_logits, sorted_idx = torch.sort(logits, descending=True, dim=-1)\n",
    "            probs = F.softmax(sorted_logits, dim=-1)\n",
    "            cum = torch.cumsum(probs, dim=-1)\n",
    "            mask = cum > top_p\n",
    "            mask[..., 1:] = mask[..., :-1].clone()\n",
    "            mask[..., 0] = False\n",
    "            sorted_logits = sorted_logits.masked_fill(mask, float(\"-inf\"))\n",
    "            logits = torch.full_like(logits, float(\"-inf\"))\n",
    "            logits.scatter_(1, sorted_idx, sorted_logits)\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_tok = torch.multinomial(probs, num_samples=1)\n",
    "        x = torch.cat([x, next_tok], dim=1)\n",
    "        nid = next_tok.item()\n",
    "        collected_new_ids.append(nid)\n",
    "\n",
    "        if eos_id is not None and nid == eos_id:\n",
    "            break\n",
    "\n",
    "        # 流式打印新 token\n",
    "        if tokenizer is not None and hasattr(tokenizer, \"decode\") and ((t+1) % print_every == 0):\n",
    "            # 只解码新 token（简单做法：解码全序列最后若干；更省事直接 decode 全部也行）\n",
    "            print(tokenizer.decode([nid]), end=\"\", flush=True)\n",
    "\n",
    "    # 最后收尾\n",
    "    out_ids = x[0].tolist()\n",
    "    text = tokenizer.decode(out_ids) if (tokenizer is not None and hasattr(tokenizer, \"decode\")) else str(out_ids)\n",
    "    print()  # 换行\n",
    "    return text, out_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c51fd062-6656-4f63-ba26-947fdaf83c99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T00:58:13.900032Z",
     "iopub.status.busy": "2025-09-09T00:58:13.899798Z",
     "iopub.status.idle": "2025-09-09T00:58:17.060697Z",
     "shell.execute_reply": "2025-09-09T00:58:17.060383Z",
     "shell.execute_reply.started": "2025-09-09T00:58:13.900019Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: hi\n",
      "I love you!: I have a bounder hangmen a\n",
      "more-honest will a loss as you are gone.\n",
      "\n",
      "POLIXENES:\n",
      "If you fear you, promises so boy.\n",
      "\n",
      "POLIXENES:\n",
      "I do, a soon.\n",
      "\n",
      "MARCIUS:\n",
      "True, no more of that I was much in him, the\n",
      "lips to gett him in him that sit this allieve.\n",
      "\n",
      "ANGELO:\n",
      "When is here? what art thou? thy thricing is the\n",
      "two large a worthily complainanus?\n",
      "\n",
      "ISABELLA:\n",
      "Go to, get your faith, I did in the last\n",
      "white of you are good to me, therefore to stay\n",
      "so so young.\n",
      "\n",
      "LEONTES:\n",
      "But, I warrant to stay; I am and the more.\n",
      "\n",
      "First too the boy: she straight, let's have mad me be\n",
      "me meed to my lord. If he die, I saw\n",
      "here sworn it.\n",
      "\n",
      "CORIOLANUS:\n",
      "True, sir, and my true scons.\n",
      "\n",
      "COMINIUS:\n",
      "Well, thou wilt me be alowed\n",
      "Before I shall lack the thine of thy coat,\n",
      "That his to make me hour with thee.\n",
      "\n",
      "MARCIUS:\n",
      "Come, there both: go you are more between like\n",
      "The sain of their way of my son, I come to thee with\n",
      "thee, let me stay not to stay awhile; therefore\n",
      "shall be short stroke upon my fair brother,\n",
      "With my fair son presermine \n"
     ]
    }
   ],
   "source": [
    "_ = stream_generate(\n",
    "    model, tokenizer,\n",
    "    prompt=\"User: hi\\nI love you!:\",\n",
    "    max_new_tokens=1000, temperature=0.8, top_k=50, top_p=0.9,\n",
    "    repetition_penalty=1.05, eos_id=getattr(tokenizer, \"eos_id\", None),\n",
    "    device=\"cuda\", print_every=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ce46b7-2d04-4874-af5d-923a60c13ac2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py12torch2cuda",
   "language": "python",
   "name": "py12torch2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
